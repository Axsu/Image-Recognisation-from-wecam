{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sslindia\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# import the related module abs\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model\n",
    "#loading the saved model for use\n",
    "model1 = load_model('weightmodel1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the list of category for maping the result \n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the system camera for click the image \n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come her\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "#resize_img = cv2.resize(img  , (28 , 28))        \n",
    "cv2.imwrite('image.jpg', frame)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xefd95c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf23f940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# changing the image shape to the MNIST data set \n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imageprepare(argv):\n",
    "    \"\"\"\n",
    "    This function returns the pixel values.\n",
    "    The imput is a png file location.\n",
    "    \"\"\"\n",
    "    im = Image.open(argv).convert('L')\n",
    "    width = float(im.size[0])\n",
    "    height = float(im.size[1])\n",
    "    newImage = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
    "\n",
    "    if width > height:  # check which dimension is bigger\n",
    "        # Width is bigger. Width becomes 20 pixels.\n",
    "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
    "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
    "            nheight = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
    "        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n",
    "    else:\n",
    "        # Height is bigger. Heigth becomes 20 pixels.\n",
    "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
    "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
    "            nwidth = 1\n",
    "            # resize and sharpen\n",
    "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
    "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
    "        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
    "\n",
    "    # newImage.save(\"sample.png\n",
    "\n",
    "    tv = list(newImage.getdata())  # get pixel values\n",
    "\n",
    "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
    "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
    " \n",
    "    return tva\n",
    "\n",
    "x=imageprepare('image.jpg')#file path here\n",
    "myarray = np.asarray(x)\n",
    "\n",
    "myarray=myarray.reshape(28,28)\n",
    "myarray\n",
    "plt.imshow(myarray)\n",
    "# mnist IMAGES are 28x28=784 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.expand_dims(myarray, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.9826976e-03, 8.5267516e-07, 4.2997904e-02, 2.1750382e-05,\n",
       "       2.1752760e-02, 1.3011574e-05, 2.8760733e-02, 2.1848300e-05,\n",
       "       8.9944845e-01, 2.9030280e-08], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bag'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label = np.argmax(predictions[0])\n",
    "class_names[predicted_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
